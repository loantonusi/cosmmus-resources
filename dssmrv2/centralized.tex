%!TEX root =  main.tex
\section{\dynastar: dynamic and quasi-optimum state partitioning}
%\section{\dynastar: dynamic partitioning for scalable state machine replication}
%\section{The centralized partitioning scheme}

%If the system can be modelled as a graph, as described in the previous session, we can take advantage of algorithms that perform graph partitioning to optimise the state partitioning of \ssmr\  while using concepts from \dssmr. The problem of graph partitioning is well stablished and despite being NP-Complete \ref{NPC_GraphPartition}, several approximation algorithms exist. First we define what graph partitioning is and how it can analysed to our needs, next we explore possibilities that are easily obtainable when applying the same graph's reasoning.

\subsection{Overview}
%\subsection{State partitioning as a graph problem}

We represent a service workload as a graph $G = (V, E)$, where vertices are state objects and an edges are operations.
There is an edge connecting two objects in the graph if a client can issue a command that accesses the objects. 




\subsection{The \dynastar protocol}

%\input{algorithm_oracle_partition}

\begin{figure*}
\begin{minipage}[b]{1\linewidth} % A minipage that covers the whole width of the page
\centering
      \includegraphics[width=1.0\linewidth]{figures/new-scheme}
\end{minipage}
\caption{How commands are treated in case of a repartitioning in the oracle.}
\label{fig:oracle_repartition}
\end{figure*}

%\begin{figure*}
%\begin{minipage}[b]{1\linewidth} % A minipage that covers the whole width of the page
%\centering
%      \includegraphics[width=1.0\linewidth]{figures/repartitioning}
%\end{minipage}
%\caption{How commands are treated in case of a repartitioning in the oracle.}
%\label{fig:oracle_repartition}
%\end{figure*}


When issuing a command, the application simply forwards the command to the client proxy and waits for the reply.
Consulting the oracle and multicasting the command to different partitions is done internally by the proxy at the client.
Algorithms~\ref{alg:client_proxy}, \ref{alg:server_proxy}, and \ref{alg:oracle_proxy} describe in detail how the \dssmr\ proxy works respectively at client, server and oracle processes.
Every server proxy at a server in $\ssm_i$ has only partial knowledge of the partitioning: it knows only which variables belong to $\ppm_i$.
The oracle proxy has knowledge of every $\ppm \in \Psi$.
To maintain such a global knowledge, the oracle must \amdel{} every command that creates, moves, or deletes variables.
(In Section~\ref{sec:optm}, we introduce a caching mechanism to prevent the oracle from becoming a performance bottleneck.)

\clearpage
\input{algorithm_client_proxy}
\input{algorithm_server_proxy}
\input{algorithm_oracle_proxy}



\textbf{The client proxy.} To execute a command $C$, the proxy first consults the oracle.
The oracle knows all state variables and which partition contains each of them.
Because of this, the oracle may already tell the client whether the command can be executed or not.
Such is the case of the $access(\omega)$ command: if there is a variable $v \in \omega$ that the command tries to read or write and $v$ does not exist, the oracle already tells the client that the command cannot be executed, by sending $nok$ as the prophecy.
A $nok$ prophecy is also returned for a $create(v)$ command when $v$ already exists.
For a $delete(v)$ command when $v$ already does not exist, an $ok$ prophecy is returned.
If the command can be executed, the client proxy receives a prophecy containing a pair $\langle v, \ppm \rangle$, for every variable $v$ created, accessed or deleted by the command.
If the prophecy regarding an $access(\omega)$ command contains multiple partitions, the client proxy chooses one of them, $\ppm_d$, and tries to move all variables in $\omega$ to $\ppm_d$.
Then, the command $C$ itself is multicast to $\ppm_d$.
As discussed in Section~\ref{sec:generalidea}, there is no guarantee that an interleave of commands will not happen, even if the client waits for the replies to the move commands.
For this reason, and to save time, the client proxy multicasts all move commands at once.
Commands that change the partitioning (i.e., create and delete) are also multicast to the oracle.
If the reply received to the command is $retry$, the procedure restarts: the proxy consults the oracle again, possibly moves variables across partitions, and multicasts $C$ to the appropriate partitions once more.
After reaching a given threshold of retries for $C$, the proxy falls back to \ssmr{}, multicasting $C$ to all partitions (and the oracle, in case $C$ is a create or delete command), which ensures the command's termination.


\textbf{The server proxy.} Upon delivery, access commands are intercepted by the \dssmr\ proxy before they are executed by the application server.
In \dssmr{}, every access command is executed in a single partition.
If a server proxy in partition $\ppm$ intercepts an $access(\omega)$ command that accesses a variable $v \in \omega$ that does not belong to $\ppm$, it means that the variable is in some other partition, or it does not exist.
Either way, the client should retry with a different set of partitions, if the variable does exist.
To execute a $delete(v)$ command, the server proxy at partition $\ppm$ simply removes $v$ from partition $\ppm$, in case $v \in \ppm$.
In case $v \not\in \ppm$, it might be that the variable exists but belongs to some other partition $\ppm'$.
Since only the oracle and the servers at $\ppm'$ have this knowledge, it is the oracle who replies to delete commands.

\dssmr\ server and oracle proxies coordinate to execute commands that create or move variables.
Such coordination is done by means of \rmcast{}.
When a $create(v)$ command is delivered at $\ppm$, the server proxy waits for a message from the oracle, telling whether the variable can be created or not, to be \rmdel{}ed.
Such a message from the oracle is necessary because $v$ might not belong to $\ppm$, but it might belong to some other partition $\ppm'$ that servers of $\ppm$ have no knowledge of.
If the create command can be executed, the oracle can already reply to the client with a positive acknowledgement, saving time.
This can be done because atomic multicast guarantees that all non-faulty servers at $\ppm$ will eventually deliver and execute the command.
As for move commands, each $move(v,\ppm_s,\ppm_d)$ command consists of moving variable $v$ from a source partition $\ppm_s$ to a destination partition $\ppm_d$.
If the server's partition $\ppm$ is the source partition (i.e., $\ppm$ = $\ppm_s$), the server proxy checks whether $v$ belongs to $\ppm$.
If $v \in \ppm$, the proxy \rmcast{}s $\langle v, C \rangle$ to $\ppm_d$, so that servers at the destination partition know the most recent value of $v$; $C$ is sent along with $v$ to inform which move command that message is related to.
If $v \not\in \ppm$, a $\langle null, C \rangle$ message is \rmcast\ to $\ppm_d$, informing $\ppm_d$ that the move command cannot be executed.



\textbf{The oracle proxy.} One of the purposes of the oracle proxy is to make prophecies regarding the location of state variables.
Such prophecies are used by client proxies to multicast commands to the right partitions.
A prophecy regarding an $access(\omega)$ command contains, for each $v \in \omega$, a pair $\langle v, \ppm \rangle$, meaning that $v \in \ppm$.
If any of the variables in $\omega$ does not exist, the prophecy already tells the client that the command cannot be executed (with a $nok$ value).
For a $create(v)$ command, the prophecy tells where $v$ should be created, based on rules defined by the application, if $v$ does not exist.
If $v$ already exists, the prophecy will contain $nok$, so that the client knows that the create command cannot be executed.
The prophecy regarding a $delete(v)$ command contains the partition that contains $v$, or $ok$, in case $v$ was already deleted or never~existed.

Besides dispensing prophecies, the oracle is responsible for executing create, move, and delete commands, coordinating with server proxies when necessary, and replying directly to clients in some cases.
For each $move(v,\ppm_s,\ppm_d)$ command, the oracle checks whether $v$ in fact belongs to the source partition $\ppm_s$.
If that is the case, the command is executed, moving $v$ to $\ppm_d$.
Each $create(v)$ command is multicast to the oracle and to a partition $\ppm$.
If $v$ already exists, the oracle tells $\ppm$ that the command cannot be executed, by \rmcast{}ing $nok$ to $\ppm$.
The oracle also sends $nok$ to the client as reply, meaning that $v$ already exists.
If $v$ does not exist, the oracle tells $\ppm$ that the command can be executed, by \rmcast{}ing $ok$ to $\ppm$.
It also tells the client that the command succeeded with an $ok$ reply.
Finally, each $delete(v)$ command is multicast to the oracle and to a partition $\ppm$, where the client proxy assumed $v$ to be located.
If $v$ belongs to $\ppm$, or $v$ does not exist, the oracle tells the client that the delete command succeeded.
Otherwise, that is, if $v$ exists, but $delete(v)$ was multicast to the wrong partition, the oracle tells the client to retry.



\subsection{Performance optimizations}
\label{sec:optm}

In this section, we introduce two optimizations for \dssmr{}: caching and load balancing.

\textbf{Caching.} In Algorithm~\ref{alg:client_proxy}, for every command issued by the client, the proxy consults the oracle.
If every command passes by the oracle, the system is unlikely to scale, as the oracle is prone to becoming a bottleneck.
To provide a scalable solution, each client proxy has a local cache of the partitioning information.
Before multicasting an application command $C$ to be executed, the client proxy checks whether the cache has information about every variable concerned by $C$.
If the cache does have such a knowledge, the oracle is not consulted and the information contained in the cache is used instead.
If the reply to $C$ is $retry$, the oracle is consulted and the returned prophecy is used to update the client proxy's cache.
Algorithm~\ref{alg:client_proxy} is followed from the second attempt to execute $C$ on.
The cache is a local service that follows an algorithm similar to that of the oracle, except it responds only to $consult(C)$ commands and, in situations where the oracle would return $ok$ or $nok$, the cache tells the client proxy to consult the actual oracle.



%\begin{figure}
%  \includegraphics[width=\linewidth]{figures/cache_retry}
%  \caption{A cache is used by each client to avoid consulting the oracle.}
%  \label{fig:cache_retry}
%\end{figure}

Naturally, the cached partitioning information held by the client proxy may be out of date.
On the one hand, this may lead a command to be multicast to the wrong set of partitions, which will probably incur in the client proxy having to retry executing the command.
For instance, in Figure~\ref{fig:cache_retry} the client has an out-of-date cache, incurring in a new consultation to the oracle when executing $C_3$.
On the other hand, the client proxy may already have to retry commands, even if the oracle is always consulted first, as shown in Figure~\ref{fig:move_case_1}.
If most commands are executed without consulting the oracle, as in the case of $C_4$ in Figure~\ref{fig:cache_retry}, we avoid turning the oracle into a bottleneck.
Moreover, such a cache can be updated ahead of time, not having to wait for an actual application command to be issued to only then consult the oracle.
This way, the client proxy can keep a cache of partitioning information of variables that the proxy deems likely to be accessed in the future.

\textbf{Load balancing}. When moving variables, the client proxies may try to distribute them in a way that balances the workload among partitions.
This way, the system is more likely to scale throughput with the number of server groups.
One way of balancing load is by having roughly the same number of state variables in every partition.
This can be implemented by having client proxies choosing randomly the partition that will receive all variables concerned by each command (at line~\ref                {algline:client:partition} of Algorithm~\ref{alg:client_proxy}).
Besides improving performance, balancing the load among partitions prevents the system from degenerating into a single-partition system, with all variables being moved to the same place as commands are executed.




\input{correctness}




