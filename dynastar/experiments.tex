%!TEX root =  main.tex
\section{Performance evaluation}
\label{sec:experiments}

In this section, we evaluate \dynastar{} according to various metrics,
under a variety of different operational parameters.  As a
representative application, we use the \appname{} social networking
service described in the previous section.  As a point of comparison,
we perform the same experiments with both
\ssmr{}~\cite{bezerra2014ssmr} and \dssmr. Our experiments show that
\dynastar{} is able to rapidly adapt to changing workloads, while
achiving throughputs and latencies far better than the existing
state-of-the-art approaches to partitioning.


\subsection{Experiemental environment}
\label{sec:evaluation:setup}

We conducted all experiments on a cluster with two types of nodes: (a)
HP SE1102 nodes, equipped with two Intel Xeon L5420 processors running
at 2.5 GHz and with 8 GB of main memory, and (b) Dell SC1435 nodes,
equipped with two AMD Opteron 2212 processors running at 2.0 GHz and
with 4 GB of main memory. The HP nodes were connected to an HP
ProCurve 2920-48G gigabit network switch, and the Dell nodes were
connected to another, identical switch. Those switches were
interconnected by a 20 Gbps link.  All nodes ran CentOS Linux 7.1 with
kernel 3.10 and had the OpenJDK Runtime Environment~8 with the
\mbox{64-Bit} Server VM (build 25.45-b02).


\subsection{Methodology and goals}
\label{sec:evaluation:methodology}

The experiments explore the following questions:
\begin{itemize}
\item \emph{How does \dynastar compare to other approaches?} 
\item \emph{How does the number of partitions affect the performance of posts for a fixed social graph?}
\item \emph{How does \dynastar perform under dynamic workloads?}
\item \emph{When will the oracle become a bottleneck?}
\end{itemize}



\paragraph*{Social network graph creation}
%
To generate the social graphs used in the experiments, we used the
Holme-Kim model~\cite{holme-kim}.  We created power-law graphs (also
known as scale-free graphs) with a clustering coefficient varying from
0.6 to 1, which represent the geometric structures of social
networks. The coefficient is the probability that whenever an edge
$(v, w)$ is added to the graph, $v$ connects to some neighbour of $w$.
In the experiments, we tested graphs with 10000 users. We characterize
different workloads by varying the percentage of edge-cuts.  For
example, a graph with a 5\% edge cut means that 5\% of the total edges
in the graph have connected vertices located in different partitions.
Graphs with larger edge-cut percentages have \emph{weak locality}.

\paragraph*{Command generation}
%
As a workload for the social graph, clients issued a sequence of post
commands.  For each command, the client selected a node a random as
the poster.  We focused on post commands, since they may access
multiple partitions.  We used 100 clients per partition. In other
words, the number of clients varied linearly with the number of
partitions.  Each client repeatedly issued synchronous post commands,
waiting for a response from the storage.

\paragraph*{Performance metrics}
%
The latency was measured as the end-to-end time between issuing the
command, and receiving the response.  Throughput was measured as the
number of posts/second that the clients were able to send.

\paragraph*{Operational parameters}
%
With post commands, the frequency of single-partition
vs. multi-partition commands depends on the number of partitions, the
geometry of the social graph, and the technique used to partition the
data. We ran experiments with 2, 4, and 8 partitions.  To vary the
geometry, we generated graphs with varying percentages of edge-cuts as
computed by METIS on a static graph. Our experiments used graphs with
0\%, 1\%, 5\% and 10\% of edge cuts. As already mentioned, we used
three partitioning strategies: \ssmr{}, \dssmr, and \dynastar.


\subsection{\dynastar vs. alternative systems}
\label{sec:evaluation:results}

To compare \dynastar with alternative approaches, we measured the
throughput and latency for a varying number of partitions and a
varying percentage of edge-cuts. The results are shown in
Figure~\ref{fig:varying_edge_cut}.

As expected, all three techniques perform similarly on tests with
strong locality, because there are no cross-partition commands after
the graph is perfectly partitioned and no more moves occur in
\dynastar or \dssmr.  Also, no synchronization among partitions is
necessary for \ssmr.

However, as the percentage of edge-cuts increases, \dssmr\ performance
decreases significantly.  This happens because with weak locality,
objects in \dssmr\ are constantly being moved back and forth between
partitions, without converging to a stable configuration. In contrast, for \dynastar and \ssmr with
static partitioning, we see that the throughput scales with the
number of partitions, up to a point that the number of moves  and 
cross-partition commands have a considerable overhead in the execution.
\rjs{I don't understand the previous sentence.}

\begin{figure*}[ht!]
	\includegraphics{figures/experiments/throughput-latency-avg-all}
	\caption{Throughput and latency, varying edge-cuts for different partitioning size}
	\label{fig:varying_edge_cut}
\end{figure*}


\subsection{Performance with the number of partitions}
\label{sec:evaluation:results}
In this tests we measure the throughput of the same graph partitioned in different
partition sizes. As seen in Figure~\ref{fig:4p1p_varying_partition_size}, the throughput
scales with the number of partitions up until a point, then decreases, that is because
when increasing the partitioning size, the number of edge-cuts also increase, in this
case, the number of edge-cuts for 2, 4, 6 and 8 partitions were respectively 
0.13\%, 1.06\%, 2.28\% and 2.67\%. \ef{Is that reason enough? It is a reason but
I think is not the only thing}

\begin{figure}[ht]
	\includegraphics{figures/experiments/throughput-avg-vary-partition}
	\caption{Same graph in different partitioning}
	\label{fig:4p1p_varying_partition_size}
\end{figure}


\subsection{Performance under dynamic workloads}

Figure~\ref{fig:dynamic_load_tput} depicts dynamically repartitioning
on-the-fly.  We started the system with an empty graph. Then clients
continuously create users and links between them during the experiment
(running the follow command).  The oracle monitors changes in the
graph's structure and trigger a repartitioning when the number of
changes exceed a threshold.  Each time the repartitioning took place,
the partitioning became better, that helps the throughput increase.

\begin{figure}[ht]
	\includegraphics{figures/experiments/dynamicload-tp-move-4p}
	\caption{Adding nodes and repartitioning dynamically}
	\label{fig:dynamic_load_tput}
\end{figure}

\subsection{The performance of the oracle}

\dynastar differs from \dssmr in that it uses a centralized oracle
that maintains a global view of the workload graph. This oracle allows
\dynastar to make better choices about data movement, resulting in overall
better throughput and lower latency. However, introducing centralized
components in a distributed system is always a cause for some skepticism,
in case the component becomes a bottleneck. We therefore conducted two
experiments to evaluate if the \dynastar oracle is a bottleneck to
system performance. The results show that the load on the oracle is
low, suggesting that \dynastar scales well.


The first experiment assesses the scalability of the static METIS algorithm
in isolation. We measured the time to compute the partitioning solution, and
the memory usage of the algorithms for increasingly large graphs. 
The results, shown in Figure~\ref{fig:metis_size_time}, shows that METIS scales
linearly in both memory and computation time on graphs of up to 10 million vertices.


\begin{figure}[ht!]
  \centering
    \includegraphics[width=\columnwidth]{figures/metis_size_time}
	\caption{METIS processor and memory usage}
	\label{fig:metis_size_time}
\end{figure}

The second experiment evaluates the oracle in terms of CPU load over
time, for varying numbers of partitions. The results are shown in
Figure~\ref{fig:cpu_oracle}. The plot shows that load is higher in the
beginning of the experiment, when the clients had not yet cached the
requests. However, the load diminishes rapidly, and remains relatively
low over time. This is because access to the oracle is necessary only
when clients have an invalid log or when a repartition happens. This experiments
suggest that the oracle would not become a bottleneck for reasonably large
deployments.

\begin{figure}[ht]
	\includegraphics{figures/experiments/oracle-load}
	\caption{CPU load in the oracle}
	\label{fig:cpu_oracle}
\end{figure}

