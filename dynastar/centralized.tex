%!TEX root =  main.tex
\section{\dynastar: dynamic and quasi-optimum state partitioning}
%\section{\dynastar: dynamic partitioning for scalable state machine replication}
%\section{The centralized partitioning scheme}

%If the system can be modeled as a graph, as described in the previous session, we can take advantage of algorithms that perform graph partitioning to optimise the state partitioning of \ssmr\  while using concepts from \dssmr. The problem of graph partitioning is well stablished and despite being NP-Complete \ref{NPC_GraphPartition}, several approximation algorithms exist. First we define what graph partitioning is and how it can analyzed to our needs, next we explore possibilities that are easily obtainable when applying the same graph's reasoning.

\subsection{Overview}
%\subsection{State partitioning as a graph problem}

We represent a service workload as a graph $G = (V, E)$, where vertices are state objects and an edges are operations.
There is an edge connecting two objects in the graph if a client can issue a command that accesses the objects. 




\subsection{The \dynastar protocol}

Algorithms~\ref{alg:client_proxy}, \ref{alg:server_proxy}, and \ref{alg:oracle_proxy} describe in detail how client, server and oracle processes execute, respectively.
For brevity, we omit the delete command since the coordination involved in the execution of a create and of a delete commands are analogous. 
Moreover, in the discussion in this section, every command involves the oracle.
In the next section, we explain how clients can use a caching technique to avoid using the oracle in the execution of most commands.

\begin{figure*}
\begin{minipage}[b]{1\linewidth} % A minipage that covers the whole width of the page
\centering
      \includegraphics[width=0.9\linewidth]{figures/dynastar}
\end{minipage}
\caption{The execution of a create command and a read command in \dynastar.}
\label{fig:oracle_repartition}
\end{figure*}

%When issuing a command, the application simply forwards the command to the client proxy and waits for the reply.
%Consulting the oracle and multicasting the command to different partitions is done internally by the proxy at the client.
%Every server proxy at a server in $\ssm_i$ has only partial knowledge of the partitioning: it knows only which variables belong to $\ppm_i$.
%The oracle proxy has knowledge of every $\ppm \in \Psi$.
%To maintain such a global knowledge, the oracle must \amdel{} every command that creates, moves, or deletes variables.
%(In Section~\ref{sec:optm}, we introduce a caching mechanism to prevent the oracle from becoming a performance bottleneck.)

%\clearpage
\input{algorithm_client_proxy}
\input{algorithm_server_proxy}
\input{algorithm_oracle_proxy}



\subsubsection{The client process} 

To execute a command $C$, the client atomically multicasts $C$ to the oracle in a consult message to find out the partitions involved in $C$.
The oracle replies with a prophecy, which may already tell the client that $C$ cannot be execute (e.g., $C$ needs a variable that does not exist, $C$ tries to create a variable that already exists).
If the command can be executed, the client receives a prophecy containing a tuple $(dests, sync)$, where $dests$ is a set of the partitions the command must be atomically multicast to for execution, and $sync$ is a set with the partitions the client must coordinate with before multicasting the command.
The client must coordinate with partitions before atomically multicasting a command if the command involves variables located in multiple partitions and the oracle needs to move variables to a single partition (in which case $sync$ contains the partitions involved in the moves).

After the client atomically multicasts $C$ to the target partition, it waits for a response from the servers in the target partition.
The client must retry the command if the target partition cannot execute the command.
This happens if some of the variables needed by $C$ were moved to another partition due to the interleaving of $C$ with other commands. 
To ensure that command $C$ is eventually executed, after retrying a few times, the client falls back \ssmr{}, multicasting $C$ to all partitions (and the oracle, in case $C$ is a create or delete command).

\subsubsection{The server process} 

When a server delivers a command for execution, it first checks that all variables accessed by the command are stored locally, in which case the command is executed and the response is sent back to the client (Task 1 in Algorithm~\ref{alg:server_proxy}).
If one or more variables are not stored at the partition, the server returns a message to the client to retry the operation.
This happens if some variables have been moved to another partition due to another command. 

Upon delivering a message to move variable $v$ (Task 2), a server in the source partition checks whether $v$ is still stored locally, in which case the server reliably multicasts $v$ to the destination partition; if $v$ is no longer stored in the partition, the server sends a null message to the destination partition.
Servers in the destination partition simply wait for a message from the source partition.

When a server delivers a message to create a variable (and similarly to delete an existing variable), it coordinates with the oracle (Task 3).
The exchange of signals between the partition where the variable will be created and the oracle ensures that interleaved executions between create and delete commands will not lead to violations of linearizability.

\subsubsection{The oracle process} 

One of the purposes of the oracle proxy is to make prophecies regarding the location of state variables.
Such prophecies are used by client proxies to multicast commands to the right partitions.
A prophecy regarding an $access(\omega)$ command contains, for each $v \in \omega$, a pair $\langle v, \ppm \rangle$, meaning that $v \in \ppm$.
If any of the variables in $\omega$ does not exist, the prophecy already tells the client that the command cannot be executed (with a $nok$ value).
For a $create(v)$ command, the prophecy tells where $v$ should be created, based on rules defined by the application, if $v$ does not exist.
If $v$ already exists, the prophecy will contain $nok$, so that the client knows that the create command cannot be executed.
The prophecy regarding a $delete(v)$ command contains the partition that contains $v$, or $ok$, in case $v$ was already deleted or never~existed.

Besides dispensing prophecies, the oracle is responsible for executing create, move, and delete commands, coordinating with server proxies when necessary, and replying directly to clients in some cases.
For each $move(v,\ppm_s,\ppm_d)$ command, the oracle checks whether $v$ in fact belongs to the source partition $\ppm_s$.
If that is the case, the command is executed, moving $v$ to $\ppm_d$.
Each $create(v)$ command is multicast to the oracle and to a partition $\ppm$.
If $v$ already exists, the oracle tells $\ppm$ that the command cannot be executed, by \rmcast{}ing $nok$ to $\ppm$.
The oracle also sends $nok$ to the client as reply, meaning that $v$ already exists.
If $v$ does not exist, the oracle tells $\ppm$ that the command can be executed, by \rmcast{}ing $ok$ to $\ppm$.
It also tells the client that the command succeeded with an $ok$ reply.
Finally, each $delete(v)$ command is multicast to the oracle and to a partition $\ppm$, where the client proxy assumed $v$ to be located.
If $v$ belongs to $\ppm$, or $v$ does not exist, the oracle tells the client that the delete command succeeded.
Otherwise, that is, if $v$ exists, but $delete(v)$ was multicast to the wrong partition, the oracle tells the client to retry.



\subsection{Performance optimizations}
\label{sec:optm}

In this section, we introduce two optimizations for \dssmr{}: caching and load balancing.

In Algorithm~\ref{alg:client_proxy}, for every command issued by the client, the proxy consults the oracle.
If every command passes by the oracle, the system is unlikely to scale, as the oracle is prone to becoming a bottleneck.
To provide a scalable solution, each client proxy has a local cache of the partitioning information.
Before multicasting an application command $C$ to be executed, the client proxy checks whether the cache has information about every variable concerned by $C$.
If the cache does have such a knowledge, the oracle is not consulted and the information contained in the cache is used instead.
If the reply to $C$ is $retry$, the oracle is consulted and the returned prophecy is used to update the client proxy's cache.
Algorithm~\ref{alg:client_proxy} is followed from the second attempt to execute $C$ on.
The cache is a local service that follows an algorithm similar to that of the oracle, except it responds only to $consult(C)$ commands and, in situations where the oracle would return $ok$ or $nok$, the cache tells the client proxy to consult the actual oracle.

Naturally, the cached partitioning information held by the client proxy may be out of date.
On the one hand, this may lead a command to be multicast to the wrong set of partitions, which will probably incur in the client proxy having to retry executing the command.
For instance, in Figure~\ref{fig:cache_retry} the client has an out-of-date cache, incurring in a new consultation to the oracle when executing $C_3$.
On the other hand, the client proxy may already have to retry commands, even if the oracle is always consulted first, as shown in Figure~\ref{fig:move_case_1}.
If most commands are executed without consulting the oracle, as in the case of $C_4$ in Figure~\ref{fig:cache_retry}, we avoid turning the oracle into a bottleneck.
Moreover, such a cache can be updated ahead of time, not having to wait for an actual application command to be issued to only then consult the oracle.
This way, the client proxy can keep a cache of partitioning information of variables that the proxy deems likely to be accessed in the future.



\input{correctness}




