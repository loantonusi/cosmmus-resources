%!TEX root =  main.tex
\section{Introduction}

State machine replication (SMR) is a well-established technique to develop highly available services (e.g., \cite{Shvachko:2003,Ghemawat:2003,Burrows:2006,MacCormick:2004}).
In essence, the idea is that replicas deterministically execute the same sequence of client commands in the same order and in doing so traverse the same sequence of states and produce the same results.
State machine replication provides configurable fault tolerance in the sense that the system can be set to tolerate any number of faulty replicas.
%Increasing the number of replicas, however, will not scale performance since each replica must execute every command.
Unfortunately, increasing the number of replicas will not scale performance since each replica must execute every command.

%For many online services, caping performance is a serious drawback.
Conceptually, scalable performance can be achieved with state partitioning (e.g., \cite{facebookTAO, sciascia2012sdur, Aguilera:2007}).
Ideally, if the service state can be divided such that commands access one partition only and are equally distributed among partitions, then system throughput (i.e., the number of commands that can be executed per time unit) will increase linearly with the number of partitions.
Although promising, exploiting partitioning in SMR is challenging.
First, most applications cannot be partitioned in such a way that commands always fall within a single partition.
Therefore, a partitioning scheme must cope with multi-partition commands.
Second, determining an efficient partitioning of the state is computationally expensive and requires an accurate characterization of the workload.

There are two general solutions to handle multi-partition commands.
One solution is to weaken the guarantees of commands that involve multiple partitions (e.g., \cite{facebookTAO}).
In the context of SMR, this would mean that single-partition commands are strongly consistent (i.e., linearizable) but multi-partition commands are not.
Another solution is to provide strong consistency guarantees for both single- and multi-partition commands, at the cost of a more complex execution path for commands that involve multiple partitions.
Scalable State Machine Replication (S-SMR)~\cite{bezerra2014ssmr} is a solution in this category.
S-SMR partitions the service state and replicates each partition.
It relies on an atomic multicast primitive to consistently order commands within and across partitions. 
Single-partition commands are multicast to their concerned partition and executed just like in classical SMR.
Multi-partition commands are multicast to all involved partitions; to prevent command interleaves that violate strong consistency, S-SMR implements execution atomicity.
With execution atomicity, partitions coordinate during the execution of multi-partition commands.
Unsurprisingly, multi-partition commands are more expensive than single-partition commands, and thus, the performance of S-SMR is particularly sensitive to the way the service state is partitioned.

Determining a partitioning of the state that avoids load imbalances and favors single-partition commands requires a good understanding about the workload. 
Even if this information is available, it represents a complex optimization problem~\cite{curino2010sch,taft2014est}.
Moreover, many online applications experience variations in demand. 
These happen for a number of reasons. 
In social networks, some users may experience a surge increase in their number of followers (e.g., new ``celebrities");
workload demand may shift along the hours of the day and the days of the week; and unexpected (e.g., a video that goes viral) or planned events (e.g., a new company starts trading in the stock exchange) may lead to exceptional periods when requests increase significantly higher than in normal periods.
S-SMR assumes a static workload partitioning.
Any state reorganization requires system shutdown and manual intervention.

Given these issues, it is crucial that highly available partitioned systems be able to dynamically adapt to the workload.
In this paper, we present dynamic scalable state machine replication (DS-SMR), a technique that allows a partitioned SMR system to reconfigure its data placement on-the-fly.
DS-SMR achieves dynamic data reconfiguration without sacrificing scalability and violating the properties of classical SMR.
These requirements introduce significant challenges.
Since state variables may change location, clients must have some means to track the current location of a variable.
The scalability requirement rules out the use of a centralized oracle that clients can consult to find out the partitions a command must be multicast to.
Even if clients can determine the current location of the state variables needed by a command to execute, by the time the command is delivered at the involved partitions one or more variables may have changed their location.
Although the client can retry the command with the new partitions, how to guarantee that the command will succeed in the second attempt?
In classical SMR, every command requested by a non-faulty client always succeeds.
The second requirement imposes a similar constraint on DS-SMR.

DS-SMR has been designed to exploit workload locality.
Our scheme captures simple instantiations of locality such as when some commands tend to access the same set of state variables and more complex cases such as the structural locality in social network applications, where users with common interests have a higher probability of being interconnected in the social graph.
We adopt a simple but effective approach to state reconfiguration: whenever a command requires data from multiple partitions, the variables involved are relocated to the same partition, chosen randomly to reduce the chances of skewed load among partitions.
Although DS-SMR could use more sophisticated forms of partitioning, by analyzing the workload (e.g., \cite{curino2010sch}) or optimizing for access patterns (e.g., \cite{taft2014est}), our technique does not require any prior information about the workload and does not need on complex optimization problems.



To track object locations without compromising scalability, in addition to a centralized oracle that keeps track of variable locations, each client caches previous consults to the oracle.
As a result, the oracle is only consulted the first time a client accesses a variable of after a variable changed location.










