\section{Performance evaluation}
\label{sec:experiments}

% shouldn't we compare to Retwis as well?

In this section, we present the results found for Chirper with different loads and partitionings and compare them with the original S-SMR~\cite{bezerra2014ssmr}.
In Section~\ref{sec:evaluation:setup}, we describe the environment where we conducted our experiments.
%In Section~\ref{sec:evaluation:latency}, we evaluate how latency improves with \dssmr{}.
%In Section~\ref{sec:evaluation:scalability}, we evaluate how \dssmr\ scales throughput with an increasing number of servers.

\subsection{Environment setup and configuration parameters}
\label{sec:evaluation:setup}

We conducted all experiments on a cluster that had two types of nodes: (a) HP SE1102 nodes, equipped with two Intel Xeon L5420 processors running at 2.5 GHz and with 8 GB of main memory, and (b) Dell SC1435 nodes, equipped with two AMD Opteron 2212 processors running at 2.0 GHz and with 4 GB of main memory. The HP nodes were connected to an HP ProCurve 2920-48G gigabit network switch, and the Dell nodes were connected to another, identical switch. Those switches were interconnected by a 20 Gbps link.
All nodes ran CentOS Linux 7.1 with kernel 3.10 and had the OpenJDK Runtime Environment~8 with the \mbox{64-Bit} Server VM (build 25.45-b02).
We kept the clocks synchronized using NTP in order to measure latency components involving events in different computers.

In all our experiments, clients submit commands asynchronously and handle replies with callbacks.
We use two kinds of workloads: Timeline (all issued requests are getTimeline) and Mix (7.5\% post, 3.75\% follow, 3.75\% unfollow, and 85\% getTimeline).